{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "import gym\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Process only one image\n",
    "def phi_map(image_list):\n",
    "    # Frame Skipping size\n",
    "    k = len(image_list)\n",
    "    \n",
    "    im_tuple = tuple()\n",
    "    for i in range(k):    \n",
    "        # Load single image as PIL and convert to Luminance\n",
    "        im = PIL.Image.fromarray(image_list[i]).convert('L')\n",
    "        # Resize image\n",
    "        im = im.resize((84, 84), PIL.Image.ANTIALIAS)\n",
    "        # Transform to numpy array\n",
    "        im = np.array(im)\n",
    "        # Add processed image to tuple\n",
    "        im_tuple += (im,)\n",
    "    \n",
    "    # Return tensor of processed images\n",
    "    arr = tuple_to_numpy(im_tuple)\n",
    "    return arr\n",
    "\n",
    "def tuple_to_numpy(im_tuple):\n",
    "    # Stack tuple of 2D images as 3D np array\n",
    "    arr = np.dstack(im_tuple)\n",
    "    # Move depth axis to first index: (height, width, depth) to (depth, height, width)\n",
    "    arr = np.moveaxis(arr, -1, 0)\n",
    "    # Make arr 4D by adding dimension at first index \n",
    "    arr = np.expand_dims ( arr, 0 )\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(512, num_actions),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.hidden(x)\n",
    "        x = self.out(x)\n",
    "#         print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, N, image_shape=(4, 84, 84)):\n",
    "        self.N = N\n",
    "        # Next position in arrays to be used\n",
    "        self.index = 0\n",
    "        \n",
    "        # One np array for each tuple element\n",
    "        self.phi_t = np.zeros( (N, ) + image_shape )\n",
    "        self.action = np.zeros(N)\n",
    "        self.reward = np.zeros(N)\n",
    "        self.phi_t_plus1 = np.zeros( (N, ) + image_shape )\n",
    "        self.terminates = np.zeros(N)\n",
    "        \n",
    "        self.full = False\n",
    "    \n",
    "    def add(self, experience):\n",
    "        '''\n",
    "        This operation adds a new experience e, replacing the earliest experience if full.\n",
    "        '''\n",
    "        self.phi_t[self.index]  = experience[0]\n",
    "        self.action[self.index] = experience[1]\n",
    "        self.reward[self.index] = experience[2]\n",
    "        self.phi_t_plus1[self.index] = experience[3]\n",
    "        self.terminates[self.index] = experience[4]\n",
    "        \n",
    "        # Update value of next index\n",
    "        self.index = (self.index + 1) % self.N\n",
    "        \n",
    "        # Update 'full' when array is full\n",
    "        if not self.full and self.index == 0:\n",
    "            self.full = True\n",
    "        \n",
    "    def sample(self, size):\n",
    "        num_items = N if self.full else self.index\n",
    "        idxs = np.random.choice(num_items, size)\n",
    "        return self.phi_t[idxs], self.action[idxs], self.reward[idxs], self.phi_t_plus1[idxs], self.terminates[idxs]\n",
    "        \n",
    "        \n",
    "class History():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.max_size = 4\n",
    "        self.list = []\n",
    "    \n",
    "    def add(self, ex):\n",
    "        # Add new element if list is not full\n",
    "        if len(self.list) < self.max_size:\n",
    "            self.list.append(ex)\n",
    "            return\n",
    "        \n",
    "        # Move existing elements one index to the left\n",
    "        self.list[:-1] = self.list[1:]\n",
    "        # Add new value to last index\n",
    "        self.list[-1] = ex\n",
    "        \n",
    "    def get(self):\n",
    "        return self.list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_variable(arr):\n",
    "    return Variable(torch.from_numpy(arr).float())\n",
    "\n",
    "def initial_history(env):\n",
    "    s = env.reset()[0]\n",
    "    H = History()\n",
    "    for _ in range(H.max_size):\n",
    "        H.add(s)\n",
    "    return H\n",
    "\n",
    "def e_greedy_action(Q, phi, env, frame_count):\n",
    "    # Calculate annealed epsilon\n",
    "    initial_epsilon, final_epsilon = 1.0, .1\n",
    "    max_frames = float(1e7)\n",
    "    epsilon = max(final_epsilon, initial_epsilon - frame_count *((initial_epsilon - final_epsilon) / max_frames))\n",
    "    print('Epsilon: {}'.format(epsilon))\n",
    "    # Obtain a random value in range [0,1)\n",
    "    rand = np.random.uniform()\n",
    "    # With probability e select random action a_t\n",
    "    if rand < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    # Otherwise select a_t = argmax_a Q(phi, a)\n",
    "    else:\n",
    "        # Convert to Variable\n",
    "        phi = to_variable(phi)\n",
    "        return Q(phi).max(1)[1].data\n",
    "    \n",
    "def update_target_network(Q):\n",
    "    return copy.deepcopy(Q)\n",
    "\n",
    "def approximate_targets(phi_plus1_mb, r_mb, done_mb, Q_, gamma=0.99):\n",
    "    '''\n",
    "    gamma: future reward discount factor\n",
    "    '''\n",
    "    max_Q, argmax_a = Q_(to_variable(phi_plus1_mb)).detach().max(1)\n",
    "    # 0 if ep. teriminates at step j+1, 1 otherwise\n",
    "    terminates = to_variable(1 - done_mb)\n",
    "    return to_variable(r_mb) + (gamma * max_Q) * terminates\n",
    "\n",
    "def gradient_descent(optimizer, loss_func, y, Q, phi_mb, action_mb, mb_size):\n",
    "    # Calculate Q(phi) of actions in [action_mb]\n",
    "    q_phi = Q(to_variable(phi_mb))[np.arange(mb_size), action_mb]\n",
    "#     # Clip error to range [-1, 1]\n",
    "#     error = ( torch.clamp(y - q_phi, min=-1, max=1) )**2\n",
    "    \n",
    "    # Clear previous gradients before backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Run backward pass\n",
    "    error = loss_func(q_phi, y)\n",
    "    error.backward()\n",
    "\n",
    "    # Perfom the update\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame #: 15436\n",
      "Ep. 12 Reward: -13.0\n",
      "Eps. Rewards: [-19.0, -20.0, -21.0, -21.0, -20.0, -21.0, -19.0, -21.0, -21.0, -21.0, -20.0, -21.0]\n",
      "Epsilon: 0.99861076\n"
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = 500\n",
    "MINIBATCH_SIZE = 32\n",
    "T = 10000000\n",
    "N = int(1e7) # Replay Memory size: 1M\n",
    "C = 10000 # Target nerwork update frequency\n",
    "k = 4 # Agent History Length\n",
    "frame_count = 0\n",
    "ep_reward_list = []\n",
    "loss_func = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "# Initialize replay memory D to capacity N\n",
    "D = ReplayMemory(N)\n",
    "# Initialize action-value function Q with random weights\n",
    "Q = DeepQNetwork(6)\n",
    "optimizer = optim.RMSprop(\n",
    "    Q.parameters(), lr=0.00025, momentum=0.95, alpha=0.95, eps=.01\n",
    ")\n",
    "loss = torch.nn.MSELoss(size_average=False)\n",
    "# Initialize target action-value function Q^ with weights\n",
    "Q_ = update_target_network(Q)\n",
    "\n",
    "for ep in range(NUM_EPISODES):\n",
    "    # Initialize sequence s1 = {x1} and preprocessed sequence phi = phi(s1)\n",
    "    H = initial_history(env)\n",
    "    phi = phi_map(H.get())\n",
    "    \n",
    "    ep_reward = 0.0\n",
    "    ep_num_rewards = 0.0\n",
    "    \n",
    "    \n",
    "    for t in range(T):\n",
    "        env.render(mode='human')\n",
    "        # Select action\n",
    "        action = e_greedy_action(Q, phi, env, frame_count)\n",
    "        # Execute action a_t in emulator and observe reward r_t and image x_(t+1)\n",
    "        image, reward, done, _ = env.step(action)\n",
    "        if reward != 0:\n",
    "            ep_num_rewards += 1\n",
    "            ep_reward +=  reward #(ep_reward * (ep_num_rewards-1) + reward) / (ep_num_rewards)\n",
    "        frame_count += 1\n",
    "        if done: break\n",
    "        # Set s_(t+1) = s_t, a_t, x_(t+1) and preprocess phi_(t+1) =  phi_map( s_(t+1) )\n",
    "        H.add(image)\n",
    "        phi_prev, phi = phi, phi_map(H.get())\n",
    "        # Store transition (phi_t, a_t, r_t, phi_(t+1)) in D\n",
    "        D.add((phi_prev, action, reward, phi, done))        \n",
    "        if t % 4 == 0:\n",
    "            # Sample random minibatch of transitions ( phi_j, a_j, r_j, phi_(j+1)) from D\n",
    "            phi_mb, a_mb, r_mb, phi_plus1_mb, done_mb = D.sample(MINIBATCH_SIZE)\n",
    "            # Set y_j\n",
    "            y = approximate_targets(phi_plus1_mb, r_mb, done_mb, Q_)\n",
    "            # Perform a gradient descent step on ( y_j - Q(phi_j, a_j) )^2\n",
    "            gradient_descent(optimizer, loss_func, y, Q, phi_mb, a_mb, MINIBATCH_SIZE)\n",
    "#             raw_input('')\n",
    "        # Reset Q_\n",
    "        if t % C == 0: Q_ = update_target_network(Q)\n",
    "        # -- LOGS\n",
    "        display.clear_output(True)\n",
    "        print('Frame #: {}'.format(frame_count))\n",
    "        print('Ep. {} Reward: {}'.format(ep, ep_reward))\n",
    "        print('Eps. Rewards: {}'.format(ep_reward_list))\n",
    "        # -- \\LOGS\n",
    "        # Restart game if done\n",
    "        if done: break\n",
    "    ep_reward_list.append(ep_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
